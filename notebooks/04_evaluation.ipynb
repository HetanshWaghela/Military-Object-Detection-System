{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìà Notebook 4: Comprehensive Model Evaluation\n",
                "\n",
                "## Military Object Detection - Performance Analysis\n",
                "\n",
                "This notebook provides in-depth evaluation of trained YOLOv8 models with detailed metrics and visualizations.\n",
                "\n",
                "### Objectives:\n",
                "1. **Model Loading**: Load best trained model(s)\n",
                "2. **Validation Metrics**: Compute mAP, precision, recall\n",
                "3. **Per-Class Analysis**: Detailed class-wise performance\n",
                "4. **Confusion Matrix**: Analyze classification errors\n",
                "5. **PR Curves**: Precision-Recall analysis\n",
                "6. **Failure Analysis**: Identify model weaknesses\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ All imports successful!\n"
                    ]
                }
            ],
            "source": [
                "# Standard imports\n",
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "import json\n",
                "import warnings\n",
                "\n",
                "# Data manipulation\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.patches as patches\n",
                "import seaborn as sns\n",
                "from PIL import Image\n",
                "\n",
                "# Computer Vision\n",
                "import cv2\n",
                "\n",
                "# YAML\n",
                "import yaml\n",
                "\n",
                "# Deep Learning\n",
                "import torch\n",
                "from ultralytics import YOLO\n",
                "\n",
                "# Metrics\n",
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "\n",
                "# Progress\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "# Suppress warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Plotting style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "print(\"‚úÖ All imports successful!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìã Loaded configuration with 12 classes\n"
                    ]
                }
            ],
            "source": [
                "# Define paths\n",
                "PROJECT_ROOT = Path('..')\n",
                "DATASET_ROOT = PROJECT_ROOT / 'military_object_dataset'\n",
                "CONFIG_DIR = PROJECT_ROOT / 'config'\n",
                "MODELS_DIR = PROJECT_ROOT / 'models'\n",
                "RUNS_DIR = PROJECT_ROOT / 'runs'\n",
                "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
                "FIGURES_DIR = PROJECT_ROOT / 'figures'\n",
                "\n",
                "# Dataset paths\n",
                "VAL_IMAGES = DATASET_ROOT / 'val' / 'images'\n",
                "VAL_LABELS = DATASET_ROOT / 'val' / 'labels'\n",
                "\n",
                "# Create results directory\n",
                "RESULTS_DIR.mkdir(exist_ok=True)\n",
                "\n",
                "# Load configuration\n",
                "with open(CONFIG_DIR / 'dataset.yaml', 'r') as f:\n",
                "    dataset_config = yaml.safe_load(f)\n",
                "\n",
                "CLASS_NAMES = dataset_config['names']\n",
                "NUM_CLASSES = dataset_config['nc']\n",
                "\n",
                "print(f\"üìã Loaded configuration with {NUM_CLASSES} classes\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Trained Model(s)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîç Available Trained Models:\n",
                        "==================================================\n",
                        "   1. best_model\n",
                        "      Path: ../models/best_model.pt\n"
                    ]
                }
            ],
            "source": [
                "def find_best_models(runs_dir: Path) -> list:\n",
                "    \"\"\"\n",
                "    Find all trained model checkpoints.\n",
                "    \"\"\"\n",
                "    models = []\n",
                "    detect_dir = runs_dir / 'detect'\n",
                "    \n",
                "    if detect_dir.exists():\n",
                "        for exp_dir in detect_dir.iterdir():\n",
                "            if exp_dir.is_dir():\n",
                "                best_pt = exp_dir / 'weights' / 'best.pt'\n",
                "                if best_pt.exists():\n",
                "                    models.append({\n",
                "                        'name': exp_dir.name,\n",
                "                        'path': str(best_pt),\n",
                "                        'results_csv': exp_dir / 'results.csv'\n",
                "                    })\n",
                "    \n",
                "    # Also check models directory\n",
                "    best_model = MODELS_DIR / 'best_model.pt'\n",
                "    if best_model.exists():\n",
                "        models.append({\n",
                "            'name': 'best_model',\n",
                "            'path': str(best_model),\n",
                "            'results_csv': None\n",
                "        })\n",
                "    \n",
                "    return models\n",
                "\n",
                "# Find available models\n",
                "available_models = find_best_models(RUNS_DIR)\n",
                "\n",
                "print(\"üîç Available Trained Models:\")\n",
                "print(\"=\" * 50)\n",
                "for i, model in enumerate(available_models):\n",
                "    print(f\"   {i+1}. {model['name']}\")\n",
                "    print(f\"      Path: {model['path']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Loaded model: best_model\n"
                    ]
                }
            ],
            "source": [
                "# Load the best model (or specify which one to evaluate)\n",
                "# Default: use the first available model\n",
                "\n",
                "if available_models:\n",
                "    selected_model = available_models[0]  # Change index to select different model\n",
                "    model = YOLO(selected_model['path'])\n",
                "    print(f\"‚úÖ Loaded model: {selected_model['name']}\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No trained models found! Please run Notebook 03 first.\")\n",
                "    print(\"   Alternatively, specify a model path manually:\")\n",
                "    print(\"   model = YOLO('path/to/your/model.pt')\")\n",
                "    \n",
                "    # For demo purposes, load a pretrained model\n",
                "    # model = YOLO('yolov8n.pt')  # Uncomment to use pretrained"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Run Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîç Running validation...\n",
                        "Ultralytics 8.3.237 üöÄ Python-3.13.7 torch-2.9.1 CPU (Apple M4)\n",
                        "Model summary (fused): 72 layers, 11,130,228 parameters, 0 gradients, 28.5 GFLOPs\n",
                        "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 326.0¬±276.8 MB/s, size: 105.2 KB)\n",
                        "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/Hetansh/Github/smart-serve-hackathon-submission/military_object_dataset/val/labels.cache... 2941 images, 273 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2941/2941 8.2Mit/s 0.0s\n",
                        "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 184/184 3.2s/it 9:533.0ss\n",
                        "                   all       2941       5081      0.603       0.41      0.426      0.261\n",
                        "    camouflage_soldier        385        510      0.698      0.633      0.674       0.33\n",
                        "                weapon        222        358      0.611      0.453      0.479      0.319\n",
                        "         military_tank        938       1787      0.758      0.832      0.816      0.515\n",
                        "        military_truck         84        148      0.432      0.493      0.404      0.252\n",
                        "      military_vehicle        149        307      0.459      0.469      0.395      0.284\n",
                        "              civilian          1          1          1          0          0          0\n",
                        "               soldier        420        745      0.789      0.526      0.629      0.329\n",
                        "      civilian_vehicle         18         42      0.372      0.353      0.255      0.134\n",
                        "    military_artillery         85        117      0.674     0.0171      0.211      0.114\n",
                        "                trench          1          3          0          0          0          0\n",
                        "     military_aircraft        594       1063      0.837      0.735      0.821      0.593\n",
                        "Speed: 0.3ms preprocess, 197.2ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
                        "Saving /Users/Hetansh/Github/smart-serve-hackathon-submission/notebooks/runs/detect/val2/predictions.json...\n",
                        "Results saved to \u001b[1m/Users/Hetansh/Github/smart-serve-hackathon-submission/notebooks/runs/detect/val2\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "# Run validation on the validation set\n",
                "print(\"üîç Running validation...\")\n",
                "\n",
                "val_results = model.val(\n",
                "    data=str(CONFIG_DIR / 'dataset.yaml'),\n",
                "    batch=16,\n",
                "    imgsz=640,\n",
                "    conf=0.001,  # Low confidence for full curve\n",
                "    iou=0.6,\n",
                "    plots=True,\n",
                "    save_json=True,\n",
                "    verbose=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üìä Overall Validation Metrics:\n",
                        "==================================================\n",
                        "   mAP@0.5:      0.4258\n",
                        "   mAP@0.5:0.95: 0.2609\n",
                        "   Precision:    0.6028\n",
                        "   Recall:       0.4101\n"
                    ]
                }
            ],
            "source": [
                "# Display overall metrics\n",
                "print(\"\\nüìä Overall Validation Metrics:\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"   mAP@0.5:      {val_results.box.map50:.4f}\")\n",
                "print(f\"   mAP@0.5:0.95: {val_results.box.map:.4f}\")\n",
                "print(f\"   Precision:    {val_results.box.mp:.4f}\")\n",
                "print(f\"   Recall:       {val_results.box.mr:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Per-Class Performance Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìä Per-Class Performance (sorted by AP@0.5):\n",
                        "============================================================\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>Class ID</th>\n",
                            "      <th>Class Name</th>\n",
                            "      <th>AP@0.5</th>\n",
                            "      <th>AP@0.5:0.95</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>10</td>\n",
                            "      <td>military_aircraft</td>\n",
                            "      <td>0.8206</td>\n",
                            "      <td>0.5933</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>2</td>\n",
                            "      <td>military_tank</td>\n",
                            "      <td>0.8159</td>\n",
                            "      <td>0.5152</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>0</td>\n",
                            "      <td>camouflage_soldier</td>\n",
                            "      <td>0.6742</td>\n",
                            "      <td>0.3297</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>6</td>\n",
                            "      <td>soldier</td>\n",
                            "      <td>0.6293</td>\n",
                            "      <td>0.3289</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>1</td>\n",
                            "      <td>weapon</td>\n",
                            "      <td>0.4788</td>\n",
                            "      <td>0.3190</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>5</th>\n",
                            "      <td>3</td>\n",
                            "      <td>military_truck</td>\n",
                            "      <td>0.4035</td>\n",
                            "      <td>0.2524</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6</th>\n",
                            "      <td>4</td>\n",
                            "      <td>military_vehicle</td>\n",
                            "      <td>0.3951</td>\n",
                            "      <td>0.2841</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7</th>\n",
                            "      <td>7</td>\n",
                            "      <td>civilian_vehicle</td>\n",
                            "      <td>0.2553</td>\n",
                            "      <td>0.1342</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>8</th>\n",
                            "      <td>8</td>\n",
                            "      <td>military_artillery</td>\n",
                            "      <td>0.2109</td>\n",
                            "      <td>0.1136</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>9</th>\n",
                            "      <td>5</td>\n",
                            "      <td>civilian</td>\n",
                            "      <td>0.0000</td>\n",
                            "      <td>0.0000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>10</th>\n",
                            "      <td>9</td>\n",
                            "      <td>trench</td>\n",
                            "      <td>0.0000</td>\n",
                            "      <td>0.0000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>11</th>\n",
                            "      <td>11</td>\n",
                            "      <td>military_warship</td>\n",
                            "      <td>0.0000</td>\n",
                            "      <td>0.0000</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "    Class ID          Class Name  AP@0.5  AP@0.5:0.95\n",
                            "0         10   military_aircraft  0.8206       0.5933\n",
                            "1          2       military_tank  0.8159       0.5152\n",
                            "2          0  camouflage_soldier  0.6742       0.3297\n",
                            "3          6             soldier  0.6293       0.3289\n",
                            "4          1              weapon  0.4788       0.3190\n",
                            "5          3      military_truck  0.4035       0.2524\n",
                            "6          4    military_vehicle  0.3951       0.2841\n",
                            "7          7    civilian_vehicle  0.2553       0.1342\n",
                            "8          8  military_artillery  0.2109       0.1136\n",
                            "9          5            civilian  0.0000       0.0000\n",
                            "10         9              trench  0.0000       0.0000\n",
                            "11        11    military_warship  0.0000       0.0000"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Extract per-class metrics\n",
                "per_class_metrics = []\n",
                "\n",
                "for i in range(NUM_CLASSES):\n",
                "    per_class_metrics.append({\n",
                "        'Class ID': i,\n",
                "        'Class Name': CLASS_NAMES[i],\n",
                "        'AP@0.5': val_results.box.ap50[i] if i < len(val_results.box.ap50) else 0,\n",
                "        'AP@0.5:0.95': val_results.box.ap[i] if i < len(val_results.box.ap) else 0,\n",
                "    })\n",
                "\n",
                "per_class_df = pd.DataFrame(per_class_metrics)\n",
                "per_class_df = per_class_df.sort_values('AP@0.5', ascending=False).reset_index(drop=True)\n",
                "\n",
                "print(\"üìä Per-Class Performance (sorted by AP@0.5):\")\n",
                "print(\"=\" * 60)\n",
                "display(per_class_df.round(4))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<Figure size 1600x600 with 2 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üíæ Figure saved to: figures/12_per_class_ap.png\n"
                    ]
                }
            ],
            "source": [
                "# Visualize per-class AP\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# AP@0.5 bar chart\n",
                "colors = plt.cm.RdYlGn(per_class_df['AP@0.5'].values)\n",
                "\n",
                "bars1 = axes[0].barh(per_class_df['Class Name'], per_class_df['AP@0.5'], color=colors)\n",
                "axes[0].set_xlabel('AP@0.5', fontsize=12)\n",
                "axes[0].set_ylabel('Class', fontsize=12)\n",
                "axes[0].set_title('Per-Class AP@0.5', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xlim([0, 1])\n",
                "axes[0].axvline(0.5, color='gray', linestyle='--', alpha=0.5, label='Threshold (0.5)')\n",
                "\n",
                "# Add value labels\n",
                "for bar, val in zip(bars1, per_class_df['AP@0.5']):\n",
                "    axes[0].text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
                "                 f'{val:.3f}', va='center', fontsize=9)\n",
                "\n",
                "# AP@0.5:0.95 bar chart\n",
                "colors2 = plt.cm.RdYlGn(per_class_df['AP@0.5:0.95'].values)\n",
                "\n",
                "bars2 = axes[1].barh(per_class_df['Class Name'], per_class_df['AP@0.5:0.95'], color=colors2)\n",
                "axes[1].set_xlabel('AP@0.5:0.95', fontsize=12)\n",
                "axes[1].set_ylabel('Class', fontsize=12)\n",
                "axes[1].set_title('Per-Class AP@0.5:0.95', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xlim([0, 1])\n",
                "\n",
                "# Add value labels\n",
                "for bar, val in zip(bars2, per_class_df['AP@0.5:0.95']):\n",
                "    axes[1].text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
                "                 f'{val:.3f}', va='center', fontsize=9)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../figures/12_per_class_ap.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"üíæ Figure saved to: figures/12_per_class_ap.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üéØ Performance Summary:\n",
                        "==================================================\n",
                        "\n",
                        "‚úÖ Well-Performing Classes (AP@0.5 > 0.7): 2\n",
                        "   ‚Ä¢ military_aircraft: 0.8206\n",
                        "   ‚Ä¢ military_tank: 0.8159\n",
                        "\n",
                        "‚ö†Ô∏è Challenging Classes (AP@0.5 < 0.3): 5\n",
                        "   ‚Ä¢ civilian_vehicle: 0.2553\n",
                        "   ‚Ä¢ military_artillery: 0.2109\n",
                        "   ‚Ä¢ civilian: 0.0000\n",
                        "   ‚Ä¢ trench: 0.0000\n",
                        "   ‚Ä¢ military_warship: 0.0000\n"
                    ]
                }
            ],
            "source": [
                "# Identify best and worst performing classes\n",
                "print(\"\\nüéØ Performance Summary:\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Best classes (AP@0.5 > 0.7)\n",
                "best_classes = per_class_df[per_class_df['AP@0.5'] > 0.7]\n",
                "print(f\"\\n‚úÖ Well-Performing Classes (AP@0.5 > 0.7): {len(best_classes)}\")\n",
                "for _, row in best_classes.iterrows():\n",
                "    print(f\"   ‚Ä¢ {row['Class Name']}: {row['AP@0.5']:.4f}\")\n",
                "\n",
                "# Poor classes (AP@0.5 < 0.3)\n",
                "poor_classes = per_class_df[per_class_df['AP@0.5'] < 0.3]\n",
                "print(f\"\\n‚ö†Ô∏è Challenging Classes (AP@0.5 < 0.3): {len(poor_classes)}\")\n",
                "for _, row in poor_classes.iterrows():\n",
                "    print(f\"   ‚Ä¢ {row['Class Name']}: {row['AP@0.5']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Confusion Matrix Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "def collect_predictions(model, images_dir: Path, labels_dir: Path, conf_threshold: float = 0.25) -> tuple:\n",
                "    \"\"\"\n",
                "    Collect ground truth and predictions for confusion matrix.\n",
                "    \"\"\"\n",
                "    all_gt = []\n",
                "    all_pred = []\n",
                "    \n",
                "    image_files = list(images_dir.glob('*.jpg'))\n",
                "    \n",
                "    for img_path in tqdm(image_files[:500], desc=\"Collecting predictions\"):  # Sample for speed\n",
                "        # Get ground truth\n",
                "        label_path = labels_dir / f\"{img_path.stem}.txt\"\n",
                "        gt_classes = []\n",
                "        \n",
                "        if label_path.exists():\n",
                "            with open(label_path, 'r') as f:\n",
                "                for line in f:\n",
                "                    if line.strip():\n",
                "                        gt_classes.append(int(line.strip().split()[0]))\n",
                "        \n",
                "        # Get predictions\n",
                "        results = model.predict(str(img_path), conf=conf_threshold, verbose=False)\n",
                "        pred_classes = results[0].boxes.cls.cpu().numpy().astype(int).tolist() if len(results[0].boxes) > 0 else []\n",
                "        \n",
                "        # Match predictions to ground truth (simplified matching)\n",
                "        for gt_cls in gt_classes:\n",
                "            if gt_cls in pred_classes:\n",
                "                all_gt.append(gt_cls)\n",
                "                all_pred.append(gt_cls)  # Correct prediction\n",
                "                pred_classes.remove(gt_cls)  # Remove matched\n",
                "            else:\n",
                "                all_gt.append(gt_cls)\n",
                "                all_pred.append(-1)  # Missed (background)\n",
                "        \n",
                "        # False positives\n",
                "        for pred_cls in pred_classes:\n",
                "            all_gt.append(-1)  # Background\n",
                "            all_pred.append(pred_cls)\n",
                "    \n",
                "    return np.array(all_gt), np.array(all_pred)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîç Collecting predictions for confusion matrix...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "60749df8457248238bf329af4ca5d838",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Collecting predictions:   0%|          | 0/500 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Collect predictions for confusion matrix\n",
                "print(\"üîç Collecting predictions for confusion matrix...\")\n",
                "gt_labels, pred_labels = collect_predictions(model, VAL_IMAGES, VAL_LABELS, conf_threshold=0.25)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<Figure size 2000x800 with 4 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üíæ Figure saved to: figures/13_confusion_matrix.png\n"
                    ]
                }
            ],
            "source": [
                "# Create confusion matrix (only for valid classes, excluding background)\n",
                "mask = (gt_labels >= 0) & (pred_labels >= 0)\n",
                "gt_valid = gt_labels[mask]\n",
                "pred_valid = pred_labels[mask]\n",
                "\n",
                "if len(gt_valid) > 0:\n",
                "    # Compute confusion matrix\n",
                "    cm = confusion_matrix(gt_valid, pred_valid, labels=list(range(NUM_CLASSES)))\n",
                "    \n",
                "    # Normalize by row (recall normalization)\n",
                "    cm_normalized = cm.astype('float') / (cm.sum(axis=1, keepdims=True) + 1e-10)\n",
                "    \n",
                "    # Plot confusion matrix\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
                "    \n",
                "    # Raw counts\n",
                "    sns.heatmap(\n",
                "        cm, \n",
                "        annot=True, \n",
                "        fmt='d', \n",
                "        cmap='Blues',\n",
                "        xticklabels=[CLASS_NAMES[i] for i in range(NUM_CLASSES)],\n",
                "        yticklabels=[CLASS_NAMES[i] for i in range(NUM_CLASSES)],\n",
                "        ax=axes[0]\n",
                "    )\n",
                "    axes[0].set_xlabel('Predicted', fontsize=12)\n",
                "    axes[0].set_ylabel('Actual', fontsize=12)\n",
                "    axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
                "    axes[0].tick_params(axis='x', rotation=45)\n",
                "    axes[0].tick_params(axis='y', rotation=0)\n",
                "    \n",
                "    # Normalized\n",
                "    sns.heatmap(\n",
                "        cm_normalized, \n",
                "        annot=True, \n",
                "        fmt='.2f', \n",
                "        cmap='YlOrRd',\n",
                "        xticklabels=[CLASS_NAMES[i] for i in range(NUM_CLASSES)],\n",
                "        yticklabels=[CLASS_NAMES[i] for i in range(NUM_CLASSES)],\n",
                "        ax=axes[1],\n",
                "        vmin=0, vmax=1\n",
                "    )\n",
                "    axes[1].set_xlabel('Predicted', fontsize=12)\n",
                "    axes[1].set_ylabel('Actual', fontsize=12)\n",
                "    axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
                "    axes[1].tick_params(axis='x', rotation=45)\n",
                "    axes[1].tick_params(axis='y', rotation=0)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig('../figures/13_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"üíæ Figure saved to: figures/13_confusion_matrix.png\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Not enough valid predictions for confusion matrix\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "‚úÖ No confusions detected!\n"
                    ]
                }
            ],
            "source": [
                "confusions_df = pd.DataFrame(confusions)\n",
                "\n",
                "if len(confusions_df) > 0:\n",
                "    confusions_df = confusions_df.sort_values('Count', ascending=False).head(10)\n",
                "    print(\"\\nTop 10 Confusions:\")\n",
                "    display(confusions_df)\n",
                "else:\n",
                "    print(\"\\n‚úÖ No confusions detected!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Precision-Recall Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚ö†Ô∏è PR curve not found. It should be generated during validation.\n"
                    ]
                }
            ],
            "source": [
                "# Load PR curve data if available from validation\n",
                "pr_curve_path = Path(val_results.save_dir) / 'PR_curve.png'\n",
                "\n",
                "if pr_curve_path.exists():\n",
                "    # Display the PR curve generated by YOLO\n",
                "    img = Image.open(pr_curve_path)\n",
                "    plt.figure(figsize=(12, 10))\n",
                "    plt.imshow(img)\n",
                "    plt.axis('off')\n",
                "    plt.title('Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
                "    plt.tight_layout()\n",
                "    plt.savefig('../figures/14_pr_curves.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    print(\"üíæ Figure saved to: figures/14_pr_curves.png\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è PR curve not found. It should be generated during validation.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display other validation plots if available\n",
                "plot_files = [\n",
                "    ('F1_curve.png', 'F1 Score Curve'),\n",
                "    ('P_curve.png', 'Precision Curve'),\n",
                "    ('R_curve.png', 'Recall Curve'),\n",
                "]\n",
                "\n",
                "available_plots = []\n",
                "for filename, title in plot_files:\n",
                "    plot_path = Path(val_results.save_dir) / filename\n",
                "    if plot_path.exists():\n",
                "        available_plots.append((plot_path, title))\n",
                "\n",
                "if available_plots:\n",
                "    fig, axes = plt.subplots(1, len(available_plots), figsize=(6*len(available_plots), 5))\n",
                "    if len(available_plots) == 1:\n",
                "        axes = [axes]\n",
                "    \n",
                "    for ax, (path, title) in zip(axes, available_plots):\n",
                "        img = Image.open(path)\n",
                "        ax.imshow(img)\n",
                "        ax.axis('off')\n",
                "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig('../figures/15_metric_curves.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    print(\"üíæ Figure saved to: figures/15_metric_curves.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Detection Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_detection(model, image_path: Path, conf_threshold: float = 0.25, ax=None):\n",
                "    \"\"\"\n",
                "    Visualize model detection on an image.\n",
                "    \"\"\"\n",
                "    # Run prediction\n",
                "    results = model.predict(str(image_path), conf=conf_threshold, verbose=False)\n",
                "    \n",
                "    # Get annotated image\n",
                "    annotated = results[0].plot()\n",
                "    annotated = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
                "    \n",
                "    if ax is None:\n",
                "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
                "    \n",
                "    ax.imshow(annotated)\n",
                "    ax.axis('off')\n",
                "    \n",
                "    # Get detection info\n",
                "    num_detections = len(results[0].boxes)\n",
                "    ax.set_title(f\"{image_path.name} ({num_detections} detections)\", fontsize=10)\n",
                "    \n",
                "    return results[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<Figure size 1800x1800 with 9 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üíæ Figure saved to: figures/16_sample_detections.png\n"
                    ]
                }
            ],
            "source": [
                "# Visualize sample detections\n",
                "np.random.seed(42)\n",
                "sample_images = np.random.choice(list(VAL_IMAGES.glob('*.jpg')), size=9, replace=False)\n",
                "\n",
                "fig, axes = plt.subplots(3, 3, figsize=(18, 18))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for idx, img_path in enumerate(sample_images):\n",
                "    visualize_detection(model, img_path, conf_threshold=0.25, ax=axes[idx])\n",
                "\n",
                "plt.suptitle('Sample Detections on Validation Set', fontsize=16, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig('../figures/16_sample_detections.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"üíæ Figure saved to: figures/16_sample_detections.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Confidence Distribution Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "def collect_confidence_scores(model, images_dir: Path, n_samples: int = 200) -> dict:\n",
                "    \"\"\"\n",
                "    Collect confidence scores for all detections.\n",
                "    \"\"\"\n",
                "    confidence_by_class = {i: [] for i in range(NUM_CLASSES)}\n",
                "    \n",
                "    image_files = list(images_dir.glob('*.jpg'))[:n_samples]\n",
                "    \n",
                "    for img_path in tqdm(image_files, desc=\"Collecting confidence scores\"):\n",
                "        results = model.predict(str(img_path), conf=0.01, verbose=False)\n",
                "        \n",
                "        if len(results[0].boxes) > 0:\n",
                "            classes = results[0].boxes.cls.cpu().numpy().astype(int)\n",
                "            confs = results[0].boxes.conf.cpu().numpy()\n",
                "            \n",
                "            for cls, conf in zip(classes, confs):\n",
                "                confidence_by_class[cls].append(conf)\n",
                "    \n",
                "    return confidence_by_class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîç Analyzing confidence distributions...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b6128d493a0c45e18ad8ea730b9624cc",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Collecting confidence scores:   0%|          | 0/200 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Collect confidence scores\n",
                "print(\"üîç Analyzing confidence distributions...\")\n",
                "confidence_scores = collect_confidence_scores(model, VAL_IMAGES, n_samples=200)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<Figure size 2000x1200 with 12 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üíæ Figure saved to: figures/17_confidence_distributions.png\n"
                    ]
                }
            ],
            "source": [
                "# Visualize confidence distributions\n",
                "fig, axes = plt.subplots(3, 4, figsize=(20, 12))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for class_id in range(NUM_CLASSES):\n",
                "    ax = axes[class_id]\n",
                "    scores = confidence_scores[class_id]\n",
                "    \n",
                "    if len(scores) > 0:\n",
                "        ax.hist(scores, bins=20, color='#3498db', alpha=0.7, edgecolor='black')\n",
                "        ax.axvline(np.mean(scores), color='red', linestyle='--', \n",
                "                   label=f'Mean: {np.mean(scores):.2f}')\n",
                "        ax.axvline(0.5, color='green', linestyle=':', alpha=0.7,\n",
                "                   label='Threshold: 0.5')\n",
                "        ax.legend(fontsize=8)\n",
                "    else:\n",
                "        ax.text(0.5, 0.5, 'No detections', ha='center', va='center')\n",
                "    \n",
                "    ax.set_xlabel('Confidence')\n",
                "    ax.set_ylabel('Count')\n",
                "    ax.set_title(f\"{CLASS_NAMES[class_id]}\\n(n={len(scores)})\", fontsize=10)\n",
                "    ax.set_xlim([0, 1])\n",
                "\n",
                "plt.suptitle('Confidence Score Distribution by Class', fontsize=16, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig('../figures/17_confidence_distributions.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"üíæ Figure saved to: figures/17_confidence_distributions.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Summary Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "======================================================================\n",
                        "üìä EVALUATION SUMMARY REPORT\n",
                        "======================================================================\n",
                        "\n",
                        "üîß MODEL: best_model\n",
                        "\n",
                        "üìà OVERALL METRICS:\n",
                        "   mAP@0.5:      0.4258\n",
                        "   mAP@0.5:0.95: 0.2609\n",
                        "   Precision:    0.6028\n",
                        "   Recall:       0.4101\n",
                        "\n",
                        "üèÜ BEST PERFORMING CLASSES (AP@0.5):\n",
                        "   ‚Ä¢ military_aircraft: 0.8206\n",
                        "   ‚Ä¢ military_tank: 0.8159\n",
                        "   ‚Ä¢ camouflage_soldier: 0.6742\n",
                        "\n",
                        "‚ö†Ô∏è WORST PERFORMING CLASSES (AP@0.5):\n",
                        "   ‚Ä¢ civilian: 0.0000\n",
                        "   ‚Ä¢ trench: 0.0000\n",
                        "   ‚Ä¢ military_warship: 0.0000\n",
                        "\n",
                        "üí° OBSERVATIONS:\n",
                        "   ‚Ä¢ Average AP@0.5 across classes: 0.3903\n",
                        "   ‚Ä¢ Class with highest AP: military_aircraft\n",
                        "   ‚Ä¢ Class with lowest AP: military_warship\n",
                        "\n",
                        "üîç MINORITY CLASS PERFORMANCE:\n",
                        "   ‚Ä¢ trench: AP@0.5 = 0.0000\n",
                        "   ‚Ä¢ civilian: AP@0.5 = 0.0000\n",
                        "\n",
                        "======================================================================\n"
                    ]
                }
            ],
            "source": [
                "# Generate summary report\n",
                "print(\"=\" * 70)\n",
                "print(\"üìä EVALUATION SUMMARY REPORT\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "print(f\"\\nüîß MODEL: {selected_model['name'] if available_models else 'N/A'}\")\n",
                "\n",
                "print(\"\\nüìà OVERALL METRICS:\")\n",
                "print(f\"   mAP@0.5:      {val_results.box.map50:.4f}\")\n",
                "print(f\"   mAP@0.5:0.95: {val_results.box.map:.4f}\")\n",
                "print(f\"   Precision:    {val_results.box.mp:.4f}\")\n",
                "print(f\"   Recall:       {val_results.box.mr:.4f}\")\n",
                "\n",
                "print(\"\\nüèÜ BEST PERFORMING CLASSES (AP@0.5):\")\n",
                "for _, row in per_class_df.head(3).iterrows():\n",
                "    print(f\"   ‚Ä¢ {row['Class Name']}: {row['AP@0.5']:.4f}\")\n",
                "\n",
                "print(\"\\n‚ö†Ô∏è WORST PERFORMING CLASSES (AP@0.5):\")\n",
                "for _, row in per_class_df.tail(3).iterrows():\n",
                "    print(f\"   ‚Ä¢ {row['Class Name']}: {row['AP@0.5']:.4f}\")\n",
                "\n",
                "print(\"\\nüí° OBSERVATIONS:\")\n",
                "avg_ap = per_class_df['AP@0.5'].mean()\n",
                "print(f\"   ‚Ä¢ Average AP@0.5 across classes: {avg_ap:.4f}\")\n",
                "print(f\"   ‚Ä¢ Class with highest AP: {per_class_df.iloc[0]['Class Name']}\")\n",
                "print(f\"   ‚Ä¢ Class with lowest AP: {per_class_df.iloc[-1]['Class Name']}\")\n",
                "\n",
                "# Identify minority class performance\n",
                "minority_classes = ['trench', 'civilian']  # Known minority classes\n",
                "print(f\"\\nüîç MINORITY CLASS PERFORMANCE:\")\n",
                "for cls in minority_classes:\n",
                "    row = per_class_df[per_class_df['Class Name'] == cls]\n",
                "    if not row.empty:\n",
                "        print(f\"   ‚Ä¢ {cls}: AP@0.5 = {row.iloc[0]['AP@0.5']:.4f}\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üíæ Evaluation results saved to:\n",
                        "   ‚Ä¢ ../results/evaluation_results.json\n",
                        "   ‚Ä¢ ../results/per_class_metrics.csv\n"
                    ]
                }
            ],
            "source": [
                "# Save evaluation results\n",
                "eval_results = {\n",
                "    'model_name': selected_model['name'] if available_models else 'unknown',\n",
                "    'overall_metrics': {\n",
                "        'mAP50': float(val_results.box.map50),\n",
                "        'mAP50-95': float(val_results.box.map),\n",
                "        'precision': float(val_results.box.mp),\n",
                "        'recall': float(val_results.box.mr)\n",
                "    },\n",
                "    'per_class_metrics': per_class_df.to_dict(orient='records')\n",
                "}\n",
                "\n",
                "with open(RESULTS_DIR / 'evaluation_results.json', 'w') as f:\n",
                "    json.dump(eval_results, f, indent=2)\n",
                "\n",
                "# Save per-class metrics as CSV\n",
                "per_class_df.to_csv(RESULTS_DIR / 'per_class_metrics.csv', index=False)\n",
                "\n",
                "print(f\"üíæ Evaluation results saved to:\")\n",
                "print(f\"   ‚Ä¢ {RESULTS_DIR / 'evaluation_results.json'}\")\n",
                "print(f\"   ‚Ä¢ {RESULTS_DIR / 'per_class_metrics.csv'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "‚úÖ Evaluation Complete! Proceed to Notebook 05: Inference\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n‚úÖ Evaluation Complete! Proceed to Notebook 05: Inference\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "cv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
