{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üî¨ Notebook 6: Advanced Analysis (Going Beyond)\n",
                "\n",
                "## Military Object Detection - Deep Dive Analysis\n",
                "\n",
                "This notebook goes beyond standard metrics to provide advanced insights into model behavior.\n",
                "\n",
                "### Objectives:\n",
                "1. **Error Taxonomy**: Classify detection errors\n",
                "2. **Confidence Calibration**: Analyze prediction reliability\n",
                "3. **Object Size Analysis**: Performance by object scale\n",
                "4. **Attention Visualization**: Understand model focus areas\n",
                "5. **Class Similarity Analysis**: Which classes are confused\n",
                "6. **Robustness Analysis**: Edge case identification\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standard imports\n",
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "import json\n",
                "import warnings\n",
                "from collections import Counter, defaultdict\n",
                "\n",
                "# Data manipulation\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.patches as patches\n",
                "import seaborn as sns\n",
                "from PIL import Image\n",
                "\n",
                "# Computer Vision\n",
                "import cv2\n",
                "\n",
                "# YAML\n",
                "import yaml\n",
                "\n",
                "# Deep Learning\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "from ultralytics import YOLO\n",
                "\n",
                "# Metrics\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from scipy import stats\n",
                "from scipy.cluster.hierarchy import dendrogram, linkage\n",
                "from scipy.spatial.distance import squareform\n",
                "\n",
                "# Progress\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "# Suppress warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Plotting style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "print(\"‚úÖ All imports successful!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define paths\n",
                "PROJECT_ROOT = Path('..')\n",
                "DATASET_ROOT = PROJECT_ROOT / 'military_object_dataset'\n",
                "CONFIG_DIR = PROJECT_ROOT / 'config'\n",
                "MODELS_DIR = PROJECT_ROOT / 'models'\n",
                "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
                "FIGURES_DIR = PROJECT_ROOT / 'figures'\n",
                "\n",
                "# Dataset paths\n",
                "VAL_IMAGES = DATASET_ROOT / 'val' / 'images'\n",
                "VAL_LABELS = DATASET_ROOT / 'val' / 'labels'\n",
                "\n",
                "# Load configuration\n",
                "with open(CONFIG_DIR / 'dataset.yaml', 'r') as f:\n",
                "    dataset_config = yaml.safe_load(f)\n",
                "\n",
                "CLASS_NAMES = dataset_config['names']\n",
                "NUM_CLASSES = dataset_config['nc']\n",
                "\n",
                "print(f\"üìã Loaded configuration with {NUM_CLASSES} classes\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load best model\n",
                "best_model_path = MODELS_DIR / 'best_model.pt'\n",
                "\n",
                "if not best_model_path.exists():\n",
                "    runs_dir = PROJECT_ROOT / 'runs' / 'detect'\n",
                "    if runs_dir.exists():\n",
                "        for exp_dir in sorted(runs_dir.iterdir(), reverse=True):\n",
                "            candidate = exp_dir / 'weights' / 'best.pt'\n",
                "            if candidate.exists():\n",
                "                best_model_path = candidate\n",
                "                break\n",
                "\n",
                "if best_model_path.exists():\n",
                "    model = YOLO(str(best_model_path))\n",
                "    print(f\"‚úÖ Loaded model: {best_model_path}\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No trained model found. Using pretrained for demo.\")\n",
                "    model = YOLO('yolov8n.pt')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Error Taxonomy Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_iou(box1, box2):\n",
                "    \"\"\"\n",
                "    Calculate IoU between two boxes [x1, y1, x2, y2].\n",
                "    \"\"\"\n",
                "    x1 = max(box1[0], box2[0])\n",
                "    y1 = max(box1[1], box2[1])\n",
                "    x2 = min(box1[2], box2[2])\n",
                "    y2 = min(box1[3], box2[3])\n",
                "    \n",
                "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
                "    \n",
                "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
                "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
                "    \n",
                "    union = area1 + area2 - intersection\n",
                "    \n",
                "    return intersection / union if union > 0 else 0\n",
                "\n",
                "\n",
                "def classify_errors(gt_boxes, gt_classes, pred_boxes, pred_classes, pred_confs, iou_threshold=0.5):\n",
                "    \"\"\"\n",
                "    Classify detection errors into categories:\n",
                "    - True Positive: Correct detection\n",
                "    - Localization Error: Right class, wrong box (IoU 0.1-0.5)\n",
                "    - Classification Error: Wrong class, right location\n",
                "    - Duplicate Detection: Multiple detections of same object\n",
                "    - Background Error: Detection on background\n",
                "    - Missed Detection: Ground truth not detected\n",
                "    \"\"\"\n",
                "    errors = {\n",
                "        'true_positive': [],\n",
                "        'localization_error': [],\n",
                "        'classification_error': [],\n",
                "        'duplicate': [],\n",
                "        'background_error': [],\n",
                "        'missed': []\n",
                "    }\n",
                "    \n",
                "    gt_matched = [False] * len(gt_boxes)\n",
                "    pred_assigned = [False] * len(pred_boxes)\n",
                "    \n",
                "    # Match predictions to ground truth\n",
                "    for pi, (pred_box, pred_cls, pred_conf) in enumerate(zip(pred_boxes, pred_classes, pred_confs)):\n",
                "        best_iou = 0\n",
                "        best_gt_idx = -1\n",
                "        \n",
                "        for gi, (gt_box, gt_cls) in enumerate(zip(gt_boxes, gt_classes)):\n",
                "            iou = calculate_iou(pred_box, gt_box)\n",
                "            if iou > best_iou:\n",
                "                best_iou = iou\n",
                "                best_gt_idx = gi\n",
                "        \n",
                "        if best_iou >= iou_threshold:\n",
                "            if pred_cls == gt_classes[best_gt_idx]:\n",
                "                if gt_matched[best_gt_idx]:\n",
                "                    errors['duplicate'].append({\n",
                "                        'pred_idx': pi, 'gt_idx': best_gt_idx,\n",
                "                        'iou': best_iou, 'conf': pred_conf\n",
                "                    })\n",
                "                else:\n",
                "                    errors['true_positive'].append({\n",
                "                        'pred_idx': pi, 'gt_idx': best_gt_idx,\n",
                "                        'iou': best_iou, 'conf': pred_conf\n",
                "                    })\n",
                "                    gt_matched[best_gt_idx] = True\n",
                "            else:\n",
                "                errors['classification_error'].append({\n",
                "                    'pred_idx': pi, 'gt_idx': best_gt_idx,\n",
                "                    'pred_cls': pred_cls, 'gt_cls': gt_classes[best_gt_idx],\n",
                "                    'iou': best_iou, 'conf': pred_conf\n",
                "                })\n",
                "            pred_assigned[pi] = True\n",
                "            \n",
                "        elif best_iou >= 0.1:\n",
                "            errors['localization_error'].append({\n",
                "                'pred_idx': pi, 'gt_idx': best_gt_idx,\n",
                "                'iou': best_iou, 'conf': pred_conf\n",
                "            })\n",
                "            pred_assigned[pi] = True\n",
                "        else:\n",
                "            errors['background_error'].append({\n",
                "                'pred_idx': pi, 'conf': pred_conf\n",
                "            })\n",
                "    \n",
                "    # Find missed detections\n",
                "    for gi, matched in enumerate(gt_matched):\n",
                "        if not matched:\n",
                "            errors['missed'].append({\n",
                "                'gt_idx': gi, 'gt_cls': gt_classes[gi]\n",
                "            })\n",
                "    \n",
                "    return errors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def parse_yolo_label(label_path, img_size=640):\n",
                "    \"\"\"Parse YOLO format label file.\"\"\"\n",
                "    boxes = []\n",
                "    classes = []\n",
                "    \n",
                "    if not label_path.exists():\n",
                "        return boxes, classes\n",
                "    \n",
                "    with open(label_path, 'r') as f:\n",
                "        for line in f:\n",
                "            parts = line.strip().split()\n",
                "            if len(parts) >= 5:\n",
                "                cls = int(parts[0])\n",
                "                x_center = float(parts[1]) * img_size\n",
                "                y_center = float(parts[2]) * img_size\n",
                "                width = float(parts[3]) * img_size\n",
                "                height = float(parts[4]) * img_size\n",
                "                \n",
                "                x1 = x_center - width / 2\n",
                "                y1 = y_center - height / 2\n",
                "                x2 = x_center + width / 2\n",
                "                y2 = y_center + height / 2\n",
                "                \n",
                "                boxes.append([x1, y1, x2, y2])\n",
                "                classes.append(cls)\n",
                "    \n",
                "    return boxes, classes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Collect error statistics\n",
                "print(\"üîç Analyzing detection errors...\")\n",
                "\n",
                "all_errors = {\n",
                "    'true_positive': 0,\n",
                "    'localization_error': 0,\n",
                "    'classification_error': 0,\n",
                "    'duplicate': 0,\n",
                "    'background_error': 0,\n",
                "    'missed': 0\n",
                "}\n",
                "\n",
                "classification_errors_detail = []  # (gt_class, pred_class)\n",
                "missed_by_class = Counter()\n",
                "\n",
                "val_images = list(VAL_IMAGES.glob('*.jpg'))[:300]  # Sample for speed\n",
                "\n",
                "for img_path in tqdm(val_images, desc=\"Analyzing\"):\n",
                "    # Get ground truth\n",
                "    label_path = VAL_LABELS / f\"{img_path.stem}.txt\"\n",
                "    gt_boxes, gt_classes = parse_yolo_label(label_path)\n",
                "    \n",
                "    if len(gt_boxes) == 0:\n",
                "        continue\n",
                "    \n",
                "    # Get predictions\n",
                "    results = model.predict(str(img_path), conf=0.25, verbose=False)\n",
                "    \n",
                "    if len(results[0].boxes) > 0:\n",
                "        pred_boxes = results[0].boxes.xyxy.cpu().numpy().tolist()\n",
                "        pred_classes = results[0].boxes.cls.cpu().numpy().astype(int).tolist()\n",
                "        pred_confs = results[0].boxes.conf.cpu().numpy().tolist()\n",
                "    else:\n",
                "        pred_boxes, pred_classes, pred_confs = [], [], []\n",
                "    \n",
                "    # Classify errors\n",
                "    errors = classify_errors(gt_boxes, gt_classes, pred_boxes, pred_classes, pred_confs)\n",
                "    \n",
                "    for error_type, error_list in errors.items():\n",
                "        all_errors[error_type] += len(error_list)\n",
                "        \n",
                "        if error_type == 'classification_error':\n",
                "            for err in error_list:\n",
                "                classification_errors_detail.append((err['gt_cls'], err['pred_cls']))\n",
                "        \n",
                "        if error_type == 'missed':\n",
                "            for err in error_list:\n",
                "                missed_by_class[err['gt_cls']] += 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize error distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Error type distribution\n",
                "error_types = list(all_errors.keys())\n",
                "error_counts = list(all_errors.values())\n",
                "colors = ['#2ecc71', '#f39c12', '#e74c3c', '#9b59b6', '#e67e22', '#3498db']\n",
                "\n",
                "bars = axes[0].bar(error_types, error_counts, color=colors)\n",
                "axes[0].set_xlabel('Error Type', fontsize=12)\n",
                "axes[0].set_ylabel('Count', fontsize=12)\n",
                "axes[0].set_title('Detection Error Taxonomy', fontsize=14, fontweight='bold')\n",
                "axes[0].tick_params(axis='x', rotation=45)\n",
                "\n",
                "for bar, count in zip(bars, error_counts):\n",
                "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
                "                 str(count), ha='center', fontsize=10)\n",
                "\n",
                "# Pie chart\n",
                "total = sum(error_counts)\n",
                "percentages = [c/total*100 for c in error_counts]\n",
                "\n",
                "axes[1].pie(error_counts, labels=error_types, autopct='%1.1f%%', colors=colors,\n",
                "            explode=[0.05 if t == 'true_positive' else 0 for t in error_types])\n",
                "axes[1].set_title('Error Distribution', fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../figures/23_error_taxonomy.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"üíæ Figure saved to: figures/23_error_taxonomy.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Missed detections by class\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "missed_classes = [CLASS_NAMES[i] for i in sorted(missed_by_class.keys())]\n",
                "missed_counts = [missed_by_class[i] for i in sorted(missed_by_class.keys())]\n",
                "\n",
                "bars = ax.barh(missed_classes, missed_counts, color='#e74c3c')\n",
                "ax.set_xlabel('Number of Missed Detections', fontsize=12)\n",
                "ax.set_ylabel('Class', fontsize=12)\n",
                "ax.set_title('Missed Detections by Class', fontsize=14, fontweight='bold')\n",
                "\n",
                "for bar, count in zip(bars, missed_counts):\n",
                "    ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,\n",
                "            str(count), va='center', fontsize=10)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../figures/24_missed_by_class.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"üíæ Figure saved to: figures/24_missed_by_class.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Confidence Calibration Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def collect_calibration_data(model, images_dir, labels_dir, n_samples=200):\n",
                "    \"\"\"\n",
                "    Collect data for calibration analysis.\n",
                "    \"\"\"\n",
                "    data = {\n",
                "        'confidence': [],\n",
                "        'correct': [],\n",
                "        'class_id': []\n",
                "    }\n",
                "    \n",
                "    image_files = list(images_dir.glob('*.jpg'))[:n_samples]\n",
                "    \n",
                "    for img_path in tqdm(image_files, desc=\"Collecting calibration data\"):\n",
                "        label_path = labels_dir / f\"{img_path.stem}.txt\"\n",
                "        gt_boxes, gt_classes = parse_yolo_label(label_path)\n",
                "        \n",
                "        results = model.predict(str(img_path), conf=0.1, verbose=False)\n",
                "        \n",
                "        if len(results[0].boxes) > 0:\n",
                "            pred_boxes = results[0].boxes.xyxy.cpu().numpy()\n",
                "            pred_classes = results[0].boxes.cls.cpu().numpy().astype(int)\n",
                "            pred_confs = results[0].boxes.conf.cpu().numpy()\n",
                "            \n",
                "            for pbox, pcls, pconf in zip(pred_boxes, pred_classes, pred_confs):\n",
                "                # Check if this prediction is correct\n",
                "                is_correct = False\n",
                "                for gbox, gcls in zip(gt_boxes, gt_classes):\n",
                "                    if calculate_iou(pbox, gbox) >= 0.5 and pcls == gcls:\n",
                "                        is_correct = True\n",
                "                        break\n",
                "                \n",
                "                data['confidence'].append(float(pconf))\n",
                "                data['correct'].append(int(is_correct))\n",
                "                data['class_id'].append(int(pcls))\n",
                "    \n",
                "    return pd.DataFrame(data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Collect calibration data\n",
                "print(\"üîç Analyzing confidence calibration...\")\n",
                "calibration_df = collect_calibration_data(model, VAL_IMAGES, VAL_LABELS, n_samples=200)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate calibration curve\n",
                "def calculate_calibration_curve(df, n_bins=10):\n",
                "    \"\"\"Calculate expected accuracy vs confidence.\"\"\"\n",
                "    bins = np.linspace(0, 1, n_bins + 1)\n",
                "    \n",
                "    mean_confidence = []\n",
                "    mean_accuracy = []\n",
                "    counts = []\n",
                "    \n",
                "    for i in range(n_bins):\n",
                "        mask = (df['confidence'] >= bins[i]) & (df['confidence'] < bins[i+1])\n",
                "        if mask.sum() > 0:\n",
                "            mean_confidence.append(df.loc[mask, 'confidence'].mean())\n",
                "            mean_accuracy.append(df.loc[mask, 'correct'].mean())\n",
                "            counts.append(mask.sum())\n",
                "    \n",
                "    return mean_confidence, mean_accuracy, counts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate and plot calibration curve\n",
                "conf, acc, counts = calculate_calibration_curve(calibration_df)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Calibration curve\n",
                "axes[0].plot([0, 1], [0, 1], 'k--', label='Perfect calibration', linewidth=2)\n",
                "axes[0].scatter(conf, acc, s=[c/2 for c in counts], c='#3498db', alpha=0.7, label='Model')\n",
                "axes[0].plot(conf, acc, 'b-', alpha=0.5)\n",
                "axes[0].set_xlabel('Mean Predicted Confidence', fontsize=12)\n",
                "axes[0].set_ylabel('Fraction of True Positives', fontsize=12)\n",
                "axes[0].set_title('Calibration Curve', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xlim([0, 1])\n",
                "axes[0].set_ylim([0, 1])\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Reliability diagram with histogram\n",
                "axes[1].bar(conf, acc, width=0.08, alpha=0.7, color='#3498db', label='Accuracy')\n",
                "axes[1].plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
                "axes[1].set_xlabel('Confidence Bin', fontsize=12)\n",
                "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
                "axes[1].set_title('Reliability Diagram', fontsize=14, fontweight='bold')\n",
                "axes[1].legend()\n",
                "axes[1].set_xlim([0, 1])\n",
                "axes[1].set_ylim([0, 1])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../figures/25_calibration.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"üíæ Figure saved to: figures/25_calibration.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate Expected Calibration Error (ECE)\n",
                "total_samples = sum(counts)\n",
                "ece = sum([c * abs(a - cn) for c, a, cn in zip(counts, acc, conf)]) / total_samples if total_samples > 0 else 0\n",
                "\n",
                "print(f\"\\nüìä Calibration Metrics:\")\n",
                "print(f\"   Expected Calibration Error (ECE): {ece:.4f}\")\n",
                "print(f\"   Total predictions analyzed: {len(calibration_df)}\")\n",
                "\n",
                "# Interpretation\n",
                "if ece < 0.05:\n",
                "    print(\"   ‚úÖ Model is well-calibrated\")\n",
                "elif ece < 0.1:\n",
                "    print(\"   ‚ö†Ô∏è Model has slight calibration issues\")\n",
                "else:\n",
                "    print(\"   ‚ùå Model is poorly calibrated\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Object Size Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def collect_size_performance(model, images_dir, labels_dir, n_samples=200):\n",
                "    \"\"\"\n",
                "    Analyze performance by object size.\n",
                "    \"\"\"\n",
                "    data = {\n",
                "        'area': [],\n",
                "        'detected': [],\n",
                "        'class_id': [],\n",
                "        'size_category': []\n",
                "    }\n",
                "    \n",
                "    image_files = list(images_dir.glob('*.jpg'))[:n_samples]\n",
                "    \n",
                "    for img_path in tqdm(image_files, desc=\"Analyzing sizes\"):\n",
                "        label_path = labels_dir / f\"{img_path.stem}.txt\"\n",
                "        gt_boxes, gt_classes = parse_yolo_label(label_path)\n",
                "        \n",
                "        results = model.predict(str(img_path), conf=0.25, verbose=False)\n",
                "        \n",
                "        pred_boxes = []\n",
                "        pred_classes = []\n",
                "        if len(results[0].boxes) > 0:\n",
                "            pred_boxes = results[0].boxes.xyxy.cpu().numpy().tolist()\n",
                "            pred_classes = results[0].boxes.cls.cpu().numpy().astype(int).tolist()\n",
                "        \n",
                "        for gbox, gcls in zip(gt_boxes, gt_classes):\n",
                "            area = (gbox[2] - gbox[0]) * (gbox[3] - gbox[1])\n",
                "            \n",
                "            # Check if detected\n",
                "            detected = False\n",
                "            for pbox, pcls in zip(pred_boxes, pred_classes):\n",
                "                if calculate_iou(gbox, pbox) >= 0.5 and gcls == pcls:\n",
                "                    detected = True\n",
                "                    break\n",
                "            \n",
                "            # Categorize size (relative to 640x640)\n",
                "            img_area = 640 * 640\n",
                "            relative_area = area / img_area\n",
                "            \n",
                "            if relative_area < 0.01:\n",
                "                size_cat = 'Small (<1%)'\n",
                "            elif relative_area < 0.1:\n",
                "                size_cat = 'Medium (1-10%)'\n",
                "            else:\n",
                "                size_cat = 'Large (>10%)'\n",
                "            \n",
                "            data['area'].append(area)\n",
                "            data['detected'].append(int(detected))\n",
                "            data['class_id'].append(gcls)\n",
                "            data['size_category'].append(size_cat)\n",
                "    \n",
                "    return pd.DataFrame(data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Collect size performance data\n",
                "print(\"üîç Analyzing performance by object size...\")\n",
                "size_df = collect_size_performance(model, VAL_IMAGES, VAL_LABELS, n_samples=200)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize size performance\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Detection rate by size category\n",
                "size_order = ['Small (<1%)', 'Medium (1-10%)', 'Large (>10%)']\n",
                "size_stats = size_df.groupby('size_category')['detected'].agg(['sum', 'count', 'mean']).reset_index()\n",
                "size_stats['size_category'] = pd.Categorical(size_stats['size_category'], categories=size_order, ordered=True)\n",
                "size_stats = size_stats.sort_values('size_category')\n",
                "\n",
                "colors = ['#e74c3c', '#f39c12', '#2ecc71']\n",
                "bars = axes[0].bar(size_stats['size_category'], size_stats['mean'], color=colors)\n",
                "axes[0].set_xlabel('Object Size', fontsize=12)\n",
                "axes[0].set_ylabel('Detection Rate', fontsize=12)\n",
                "axes[0].set_title('Detection Rate by Object Size', fontsize=14, fontweight='bold')\n",
                "axes[0].set_ylim([0, 1])\n",
                "\n",
                "for bar, mean, count in zip(bars, size_stats['mean'], size_stats['count']):\n",
                "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
                "                 f'{mean:.2%}\\n(n={count})', ha='center', fontsize=10)\n",
                "\n",
                "# Scatter plot of detection vs area\n",
                "axes[1].scatter(size_df['area'], size_df['detected'], alpha=0.5, c=size_df['detected'].map({0: '#e74c3c', 1: '#2ecc71'}))\n",
                "axes[1].set_xlabel('Object Area (pixels¬≤)', fontsize=12)\n",
                "axes[1].set_ylabel('Detected (0/1)', fontsize=12)\n",
                "axes[1].set_title('Detection Success vs Object Area', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xscale('log')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../figures/26_size_analysis.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"üíæ Figure saved to: figures/26_size_analysis.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Class Similarity / Confusion Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": {},
            "source": [
                "# Create confusion similarity matrix\n",
                "if len(classification_errors_detail) > 0:\n",
                "    confusion_counts = np.zeros((NUM_CLASSES, NUM_CLASSES))\n",
                "    \n",
                "    for gt_cls, pred_cls in classification_errors_detail:\n",
                "        confusion_counts[gt_cls, pred_cls] += 1\n",
                "    \n",
                "    # Normalize\n",
                "    row_sums = confusion_counts.sum(axis=1, keepdims=True)\n",
                "    row_sums[row_sums == 0] = 1\n",
                "    confusion_normalized = confusion_counts / row_sums\n",
                "    \n",
                "    # Plot\n",
                "    fig, ax = plt.subplots(figsize=(12, 10))\n",
                "    \n",
                "    mask = confusion_counts == 0  # Mask zeros\n",
                "    \n",
                "    sns.heatmap(\n",
                "        confusion_counts,\n",
                "        annot=True,\n",
                "        fmt='.0f',\n",
                "        cmap='Reds',\n",
                "        xticklabels=[CLASS_NAMES[i] for i in range(NUM_CLASSES)],\n",
                "        yticklabels=[CLASS_NAMES[i] for i in range(NUM_CLASSES)],\n",
                "        mask=mask,\n",
                "        ax=ax\n",
                "    )\n",
                "    \n",
                "    ax.set_xlabel('Predicted Class', fontsize=12)\n",
                "    ax.set_ylabel('True Class', fontsize=12)\n",
                "    ax.set_title('Classification Confusion Matrix', fontsize=14, fontweight='bold')\n",
                "    ax.tick_params(axis='x', rotation=45)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig('../figures/27_class_confusion.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"üíæ Figure saved to: figures/27_class_confusion.png\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No classification errors found for analysis\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify most confused class pairs\n",
                "if len(classification_errors_detail) > 0:\n",
                "    print(\"\\nüîç Most Confused Class Pairs:\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    confusion_pairs = Counter(classification_errors_detail)\n",
                "    \n",
                "    for (gt_cls, pred_cls), count in confusion_pairs.most_common(10):\n",
                "        print(f\"   {CLASS_NAMES[gt_cls]:25s} ‚Üí {CLASS_NAMES[pred_cls]:25s}: {count}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Threshold Sensitivity Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_at_threshold(model, images_dir, labels_dir, conf_threshold, n_samples=100):\n",
                "    \"\"\"\n",
                "    Evaluate detection performance at a specific confidence threshold.\n",
                "    \"\"\"\n",
                "    tp, fp, fn = 0, 0, 0\n",
                "    \n",
                "    image_files = list(images_dir.glob('*.jpg'))[:n_samples]\n",
                "    \n",
                "    for img_path in image_files:\n",
                "        label_path = labels_dir / f\"{img_path.stem}.txt\"\n",
                "        gt_boxes, gt_classes = parse_yolo_label(label_path)\n",
                "        \n",
                "        results = model.predict(str(img_path), conf=conf_threshold, verbose=False)\n",
                "        \n",
                "        pred_boxes = []\n",
                "        if len(results[0].boxes) > 0:\n",
                "            pred_boxes = results[0].boxes.xyxy.cpu().numpy().tolist()\n",
                "            pred_classes = results[0].boxes.cls.cpu().numpy().astype(int).tolist()\n",
                "        else:\n",
                "            pred_classes = []\n",
                "        \n",
                "        gt_matched = [False] * len(gt_boxes)\n",
                "        \n",
                "        for pbox, pcls in zip(pred_boxes, pred_classes):\n",
                "            matched = False\n",
                "            for gi, (gbox, gcls) in enumerate(zip(gt_boxes, gt_classes)):\n",
                "                if not gt_matched[gi] and calculate_iou(pbox, gbox) >= 0.5 and pcls == gcls:\n",
                "                    tp += 1\n",
                "                    gt_matched[gi] = True\n",
                "                    matched = True\n",
                "                    break\n",
                "            if not matched:\n",
                "                fp += 1\n",
                "        \n",
                "        fn += sum(1 for m in gt_matched if not m)\n",
                "    \n",
                "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
                "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
                "    \n",
                "    return {'precision': precision, 'recall': recall, 'f1': f1}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate at different thresholds\n",
                "print(\"üîç Analyzing threshold sensitivity...\")\n",
                "\n",
                "thresholds = np.arange(0.1, 0.95, 0.1)\n",
                "threshold_results = []\n",
                "\n",
                "for thresh in tqdm(thresholds, desc=\"Evaluating thresholds\"):\n",
                "    result = evaluate_at_threshold(model, VAL_IMAGES, VAL_LABELS, thresh, n_samples=100)\n",
                "    result['threshold'] = thresh\n",
                "    threshold_results.append(result)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot threshold sensitivity\n",
                "thresh_df = pd.DataFrame(threshold_results)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "ax.plot(thresh_df['threshold'], thresh_df['precision'], 'b-o', label='Precision', linewidth=2)\n",
                "ax.plot(thresh_df['threshold'], thresh_df['recall'], 'g-o', label='Recall', linewidth=2)\n",
                "ax.plot(thresh_df['threshold'], thresh_df['f1'], 'r-o', label='F1 Score', linewidth=2)\n",
                "\n",
                "# Find optimal threshold\n",
                "optimal_idx = thresh_df['f1'].idxmax()\n",
                "optimal_thresh = thresh_df.loc[optimal_idx, 'threshold']\n",
                "optimal_f1 = thresh_df.loc[optimal_idx, 'f1']\n",
                "\n",
                "ax.axvline(optimal_thresh, color='red', linestyle='--', alpha=0.5,\n",
                "           label=f'Optimal: {optimal_thresh:.2f} (F1={optimal_f1:.3f})')\n",
                "\n",
                "ax.set_xlabel('Confidence Threshold', fontsize=12)\n",
                "ax.set_ylabel('Score', fontsize=12)\n",
                "ax.set_title('Threshold Sensitivity Analysis', fontsize=14, fontweight='bold')\n",
                "ax.legend()\n",
                "ax.grid(True, alpha=0.3)\n",
                "ax.set_xlim([0, 1])\n",
                "ax.set_ylim([0, 1])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../figures/28_threshold_sensitivity.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"üíæ Figure saved to: figures/28_threshold_sensitivity.png\")\n",
                "print(f\"\\nüìä Optimal confidence threshold: {optimal_thresh:.2f} (F1={optimal_f1:.3f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\"üî¨ ADVANCED ANALYSIS SUMMARY\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "print(\"\\nüìä ERROR TAXONOMY:\")\n",
                "total_errors = sum(all_errors.values())\n",
                "for error_type, count in all_errors.items():\n",
                "    pct = count / total_errors * 100 if total_errors > 0 else 0\n",
                "    print(f\"   {error_type:25s}: {count:5d} ({pct:5.1f}%)\")\n",
                "\n",
                "print(f\"\\nüìè SIZE-BASED PERFORMANCE:\")\n",
                "for _, row in size_stats.iterrows():\n",
                "    print(f\"   {row['size_category']:20s}: {row['mean']:.1%} detection rate (n={int(row['count'])})\")\n",
                "\n",
                "print(f\"\\nüéØ CALIBRATION:\")\n",
                "print(f\"   Expected Calibration Error: {ece:.4f}\")\n",
                "if ece < 0.05:\n",
                "    print(\"   Model confidence is well-calibrated\")\n",
                "elif ece < 0.1:\n",
                "    print(\"   Model has slight overconfidence\")\n",
                "else:\n",
                "    print(\"   Model needs confidence calibration\")\n",
                "\n",
                "print(f\"\\n‚öôÔ∏è OPTIMAL SETTINGS:\")\n",
                "print(f\"   Recommended confidence threshold: {optimal_thresh:.2f}\")\n",
                "print(f\"   Expected F1 score at optimal: {optimal_f1:.3f}\")\n",
                "\n",
                "print(\"\\nüí° RECOMMENDATIONS:\")\n",
                "if all_errors['missed'] > all_errors['background_error']:\n",
                "    print(\"   ‚Ä¢ Lower confidence threshold to reduce missed detections\")\n",
                "if all_errors['background_error'] > all_errors['missed']:\n",
                "    print(\"   ‚Ä¢ Increase confidence threshold to reduce false positives\")\n",
                "if all_errors['classification_error'] > total_errors * 0.1:\n",
                "    print(\"   ‚Ä¢ Consider class-specific training or more training data\")\n",
                "if all_errors['localization_error'] > total_errors * 0.1:\n",
                "    print(\"   ‚Ä¢ Adjust box regression loss or NMS threshold\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save analysis results\n",
                "analysis_results = {\n",
                "    'error_taxonomy': all_errors,\n",
                "    'calibration_ece': float(ece),\n",
                "    'optimal_threshold': float(optimal_thresh),\n",
                "    'optimal_f1': float(optimal_f1),\n",
                "    'size_performance': size_stats.to_dict(orient='records'),\n",
                "}\n",
                "\n",
                "with open(RESULTS_DIR / 'advanced_analysis.json', 'w') as f:\n",
                "    json.dump(analysis_results, f, indent=2)\n",
                "\n",
                "print(f\"üíæ Analysis results saved to: {RESULTS_DIR / 'advanced_analysis.json'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n‚úÖ Advanced Analysis Complete!\")\n",
                "print(\"\\nüìÅ All notebooks completed successfully!\")\n",
                "print(\"\\nüéâ Project Summary:\")\n",
                "print(\"   ‚Ä¢ Notebook 01: EDA - Dataset exploration and visualization\")\n",
                "print(\"   ‚Ä¢ Notebook 02: Preprocessing - Augmentation and class weights\")\n",
                "print(\"   ‚Ä¢ Notebook 03: Training - YOLOv8 model training pipeline\")\n",
                "print(\"   ‚Ä¢ Notebook 04: Evaluation - Comprehensive metrics and analysis\")\n",
                "print(\"   ‚Ä¢ Notebook 05: Inference - Production inference and export\")\n",
                "print(\"   ‚Ä¢ Notebook 06: Advanced - Error taxonomy and deep analysis\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}